{"cells":[{"metadata":{"_uuid":"6c2350adbb20ec2b769d7e819d89f87d62b86c59","_cell_guid":"f891b21c-43a5-46e7-93a6-84721adc91f0"},"cell_type":"markdown","source":"## **Table of Contents:**\n* Introduction\n* Exploratory data analysis\n* Data Preprocessing\n    - Converting Features\n    - Creating Categories\n    - Creating new Features\n* Building Machine Learning Models\n    - Training different models\n    - Which is the best model ?"},{"metadata":{"_uuid":"9d5cdd7511862e240273f2687c2d69f3c9bc8568","_cell_guid":"14ff033f-691f-49e3-b1b2-260b28adfe58"},"cell_type":"markdown","source":"# **Introduction**\n\nThe database we are working with classifies people into 16 distinct personality types showing their last 50 tweets, separated by \"|||\". \n\n\nMYERS BRIGGS CLASSIFICATION PROBLEM\n\nThe Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n\nIntroversion (I) – Extroversion (E)\n\nIntuition (N) – Sensing (S)\n\nThinking (T) – Feeling (F)\n\nJudging (J) – Perceiving (P)\n\n(Note that the opposite personalities are aligned above to give one a sense of difference in the meanings of the personalities while compariing them with each other.)\n\nSo for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label.\n\nIt is one of, if not the, the most popular personality test in the world. It is used in businesses, online, for fun, for research and lots more. A simple google search reveals all of the different ways the test has been used over time. It’s safe to say that this test is still very relevant in the world in terms of its use.\n\nFrom scientific or psychological perspective it is based on the work done on cognitive functions by Carl Jung i.e. Jungian Typology. This was a model of 8 distinct functions, thought processes or ways of thinking that were suggested to be present in the mind. Later this work was transformed into several different personality systems to make it more accessible, the most popular of which is of course the MBTI.\n\nRecently, its use/validity has come into question because of unreliability in experiments surrounding it, among other reasons. But it is still clung to as being a very useful tool in a lot of areas, and the purpose of this dataset is to help see if any patterns can be detected in specific types and their style of writing, which overall explores the validity of the test in analysing, predicting or categorising behaviour. Content\n\nThis dataset contains over 8600 rows of data, on each row is a person’s:\n\nType (This persons 4 letter MBTI code/type)\nA section of each of the last 50 things they have posted (Each entry separated by \"|||\" (3 pipe characters))\n![Meyer Briggs Personality Template](https://theelementsofdigitalstyle.com/wp-content/uploads/Myers-Briggs-illustration-3-2.jpg)\n\nOur goal will be to create new columns based on the content of the tweets, in order to create a predictive model. As we will see, this can be quite tricky and our creativity comes into play when analysing the content of the tweets.\n\nWe begin by importing our dataset and showing some info, for an initial exploratory analysis."},{"metadata":{"_uuid":"e1fd66d81a1166c3f8054857e1824c5de671bb09","_cell_guid":"c1984163-fffe-41aa-99a0-243673f1bf51","trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport collections\nfrom collections import Counter\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom bokeh.io import output_file, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n\ndf = pd.read_csv('../input/mbti_1.csv')\nprint(df.head(10))\nprint(\"*\"*40)\nprint(df.info())\n","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"84dec5efb299b0d3fb4988a295e8c03087539f26","_cell_guid":"fb2b5c26-42e1-4b04-8ef4-16999b6ab6f1"},"cell_type":"markdown","source":"We can thus see that there are no null inputs, which means there is no need for cleaning the data. \n\nThe first idea that pops up is checking if the words per tweet of each person shows us some information. For that reason, we can create a new column as shown below.\n"},{"metadata":{"_uuid":"ffcbdf4d43d2017e187f58a69d58cde88f5206b2","_cell_guid":"e8029602-1dc9-4a2a-b4e1-7072f81c565b","trusted":true},"cell_type":"code","source":"df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\nprint(df.head())","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d1a10157637263e2867e24564a19368ea371708a","_cell_guid":"f2a405e1-afa0-4dbb-9ee8-1164e1be07bb"},"cell_type":"markdown","source":"# **Exploratory data analysis**\n\nWe may use it for one reason or for another, but one thing we can do is printing a violin plot. \n\nAt the end I did not use it at all, but it is always nice to have the ability do some visual analysis for further investigations. "},{"metadata":{"_uuid":"903c6696d37f517ec51d35309e902e46daf2d789","_cell_guid":"29fc9dd1-39f2-49fa-8034-a887def80113","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.violinplot(x='type', y='words_per_comment', data=df, inner=None, color='lightgray')\nsns.stripplot(x='type', y='words_per_comment', data=df, size=4, jitter=True)\nplt.ylabel('Words per comment')\nplt.show()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"a72478621467a2a67d0f561a220e26af50e37ea2","_cell_guid":"08bbe211-a91c-45b3-9330-016dbecfca96"},"cell_type":"markdown","source":"There's quite a lot of information there. \n\nCreating new columns showing the amount of questionmarks per comment, exclamations or other types will be useful later on, as we will see. This are the examples I came up with, but here is where creativity comes into play.\n\nWe can also perform joint plots, pair plots and heat maps to explore relationship between data, just for fun."},{"metadata":{"_uuid":"2a92888c4fae4668421f151609c47fde41764d22","_cell_guid":"a5365240-b4f1-45b9-801f-639155108f45","trusted":true},"cell_type":"code","source":"df['http_per_comment'] = df['posts'].apply(lambda x: x.count('http')/50)\ndf['music_per_comment'] = df['posts'].apply(lambda x: x.count('music')/50)\ndf['question_per_comment'] = df['posts'].apply(lambda x: x.count('?')/50)\ndf['img_per_comment'] = df['posts'].apply(lambda x: x.count('jpg')/50)\ndf['excl_per_comment'] = df['posts'].apply(lambda x: x.count('!')/50)\ndf['ellipsis_per_comment'] = df['posts'].apply(lambda x: x.count('...')/50)\n\nplt.figure(figsize=(15,10))\nsns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df, kind='kde')","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"edf176de9553085fdecbd60a477ed77aa6f7ceeb","_cell_guid":"a14470ad-642a-4640-a64b-c6184c426f32"},"cell_type":"markdown","source":"So it seems there's a large correlation between words per comment ant the ellipsis the user types per comment! Based on pearson correlation factor there is a note of of 69% correlation between ellipsis_per_comments and words_per_comment. \n\n<center><h3> 69% ~ words correlating with ellipsis</h3> </center>\n\nThis is an interesting first step, and we are interested in taking a further look into the data as we are on a mission to discover if we can predict the personality types of users based on there social media comments with a little help from machine learning. :)"},{"metadata":{"_uuid":"2fa6ffe2f358f18a4347757907a8300239ca5f96"},"cell_type":"markdown","source":"### Exploratory Analysis Pt. 2\nWe are focusing on the correlation variables for  the different types of the personality types the Meyer Briggs outline in comparision to the words comments and ellipses per comment as well. Let's see which personality type will yield the highest correlation value. "},{"metadata":{"_uuid":"7b86180aa6b7409252aecb45ad065fd3faa49718","_cell_guid":"7820e7c7-a4c8-4249-ae87-dae21a39608f","trusted":true},"cell_type":"code","source":"i = df['type'].unique()\nk = 0\nfor m in range(0,2):\n    for n in range(0,6):\n        df_2 = df[df['type'] == i[k]]\n        sns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df_2, kind=\"hex\")\n        plt.title(i[k])\n        k+=1\n","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"831d063f8b916840403709f51736ad616aad99e8","_cell_guid":"dcdb7444-2da6-4660-acfe-5a9321a3af35","trusted":true},"cell_type":"code","source":"i = df['type'].unique()\nk = 0\nTypeArray = []\nPearArray=[]\nfor m in range(0,2):\n    for n in range(0,6):\n        df_2 = df[df['type'] == i[k]]\n        pearsoncoef1=np.corrcoef(x=df_2['words_per_comment'], y=df_2['ellipsis_per_comment'])\n        pear=pearsoncoef1[1][0]\n        print(pear)\n        TypeArray.append(i[k])\n        PearArray.append(pear)\n        k+=1\n\n\nTypeArray = [x for _,x in sorted(zip(PearArray,TypeArray))]\nPearArray = sorted(PearArray, reverse=True)\nprint(PearArray)\nprint(TypeArray)\nplt.scatter(TypeArray, PearArray)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"835fb91ef4cfcd45d3b617771e4ea25a1727bbe8"},"cell_type":"markdown","source":"# Discussion\nHighest correlation values to the ellispes and comments are as following for the top three:\n* INFJ  - Intorversion Intuition Feeling Judging \n* INTP \n* ENFP \n* "},{"metadata":{"_uuid":"32a81450ae9c91a8ff26106b8e05362543d74aff","_cell_guid":"54495ba1-ad20-4149-a843-f927c40d0f56"},"cell_type":"markdown","source":"# **Data preprocessing**\n\nTo get a further insight on our dataset, we can first create 4 new columns dividing the people by introversion/extroversion, intuition/sensing, and so on. \n\nWhen it comes to performing machine learning, trying to distinguish between two categories is much easier than distinguishing between 16 categories. We will check that later on. Dividing the data in 4 small groups will perhaps be more useful when it comes to accuracy."},{"metadata":{"_uuid":"2a2c8c86d9f4cfe5bac1a34b23eb1bc71cbaacd6","_cell_guid":"324bae84-3068-4bb6-b0ec-11a76683ddd5","trusted":true},"cell_type":"code","source":"map1 = {\"I\": 0, \"E\": 1}\nmap2 = {\"N\": 0, \"S\": 1}\nmap3 = {\"T\": 0, \"F\": 1}\nmap4 = {\"J\": 0, \"P\": 1}\ndf['I-E'] = df['type'].astype(str).str[0]\ndf['I-E'] = df['I-E'].map(map1)\ndf['N-S'] = df['type'].astype(str).str[1]\ndf['N-S'] = df['N-S'].map(map2)\ndf['T-F'] = df['type'].astype(str).str[2]\ndf['T-F'] = df['T-F'].map(map3)\ndf['J-P'] = df['type'].astype(str).str[3]\ndf['J-P'] = df['J-P'].map(map4)\nprint(df.head(10))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"1488ee7975b272f31b284e6b7e6c8580042c150d","_cell_guid":"6441ac53-9d10-4312-9191-0f0ecd440f2e"},"cell_type":"markdown","source":"# **Building machine learning algorithms**\n\nLet's do some machine learning now, first with the entire \"type\" column, with different models. Let's crank it out to the max!"},{"metadata":{"_uuid":"8c0d4b61a57e057d26dc858c317bab32d9541580","_cell_guid":"0acd5e9e-22e6-4f4f-afe0-76e2a54507e2","trusted":true},"cell_type":"code","source":"X = df.drop(['type','posts','I-E','N-S','T-F','J-P'], axis=1).values\ny = df['type'].values\n\nprint(y.shape)\nprint(X.shape)\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.1, random_state=5)\n\nsgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nsgd.score(X_train, y_train)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nprint(round(acc_sgd,2,), \"%\")","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"30168f35c770296823db30dfedad1146ee8042b7","_cell_guid":"75aa73c4-8331-41c2-b310-2e7f423611c7","trusted":true},"cell_type":"code","source":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"4a28af6cbb1041fe234a6641d49c00d60fe9f5f0","_cell_guid":"97b22da6-7fdd-4abb-8df6-f11deba405a1","trusted":true},"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"015d0834932cb68d9f4062059fe97b3d0538cf28","_cell_guid":"8d26f82f-2621-4f24-883e-8ea067759f94","trusted":true},"cell_type":"code","source":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"68ede73124edd6cfa80fe215061c8d74ac125ed8","_cell_guid":"77b27b3f-1669-47c8-9df9-756f79bc96a9"},"cell_type":"markdown","source":"## Machine Learning Discussion\nOur simple model is only able to classify people with a 24% of right guesses, which is not too much. \n\nNow we will perform machine learning with the introverted/extroverted column, and we'll see if our model is able to classify if someone is introverted or extroverted with a higher precision. So let's take a deeper dive into our model to get a better perspective on our predicting. Because this isn't going to be high enough quality to be helpful for anyone. :("},{"metadata":{"_uuid":"4ba8140f5e08778e0ab69fa000459a171c068d76","_cell_guid":"5fc456c1-0cb5-4a21-94ff-1b124e3198a6","trusted":true},"cell_type":"code","source":"XX = df.drop(['type','posts','I-E'], axis=1).values\nyy = df['I-E'].values\n\nprint(\"outcome shape\",yy.shape)\nprint(\"input shape for machine learning data\",XX.shape)\n\nXX_train,XX_test,yy_train,yy_test=train_test_split(XX,yy,test_size = 0.1, random_state=5)\n\nsgdd = SGDClassifier(max_iter=5, tol=None)\nsgdd.fit(XX_train, yy_train)\nY_predd = sgdd.predict(XX_test)\nsgdd.score(XX_train, yy_train)\nacc_sgdd = round(sgdd.score(XX_train, yy_train) * 100, 2)\nprint(round(acc_sgdd,2,), \"%\")","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"a5dc51c11b33647a4eaa5fb23a50fe3068d2715a","_cell_guid":"4a3c291d-f595-4960-9f5e-9a540d8b98a2","trusted":true},"cell_type":"code","source":"random_forestt = RandomForestClassifier(n_estimators=100)\nrandom_forestt.fit(XX_train, yy_train)\n\nY_predictionn = random_forestt.predict(XX_test)\n\nrandom_forestt.score(XX_train, yy_train)\nacc_random_forestt = round(random_forestt.score(XX_train, yy_train) * 100, 2)\nprint(\"Random Forest Predictions Model\",round(acc_random_forestt,2,), \"%\")","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"d016e4899884bd7df54f8a008ef16147828fcbae","_cell_guid":"d2d3c376-cc90-468b-9f5d-65a5262dddb7","trusted":true},"cell_type":"code","source":"# Logistic Regression\nlogregg = LogisticRegression()\nlogregg.fit(XX_train, yy_train)\n\nY_predd = logregg.predict(XX_test)\n\nacc_logg = round(logregg.score(XX_train, yy_train) * 100, 2)\nprint(\"Logisitic Regression Prediction Accuracy\",round(acc_logg,2,), \"%\")","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"adf9af8db8ec105cc00273b0b494fc0f70d0f811","_cell_guid":"bf4145c0-54d1-4706-b436-d38689791caf","trusted":true},"cell_type":"code","source":"# KNN\nknnn = KNeighborsClassifier(n_neighbors = 3)\nknnn.fit(XX_train, yy_train)\n\nY_predd = knnn.predict(XX_test)\n\nacc_knnn = round(knnn.score(XX_train, yy_train) * 100, 2)\nprint(\"Knn neighbor prediction value\",round(acc_knnn,2,), \"%\")","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"4fc8d8f3cef8422c708cd10da7b38762ab729c01","_cell_guid":"9375705f-79f3-4cb1-8d5a-16db200da250"},"cell_type":"markdown","source":"# Random Forest Prediction\n\nRandom Forest Prediction - 100 %\nLogistic Regression Prediction - 77.1 %\nKnn neighbor prediction model - 83.66 %\n\nSo we see our model has an accuracy of 77%, not bad for such a simple model! Let's see what else we can do!"},{"metadata":{"_uuid":"780fd3061b547a9c370c978648fde337bf47b332","_cell_guid":"f5feb407-e16a-4bfd-ab8c-90426336d3b8","trusted":true},"cell_type":"code","source":"new_column=[]\nfor z in range(len(df['posts'])):\n    prov=df['posts'][z]\n    prov2= re.sub(r'[“€â.|,?!)(1234567890:/-]', '', prov)\n    prov3 = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', prov)\n    prov4 = re.sub(r'[|||)(?.,:1234567890!]',' ',prov3)\n    prov5 = re.sub(' +',' ', prov4)\n    prov6 = prov5.split(\" \")\n    counter = Counter(prov6)\n    counter2 = counter.most_common(1)[0][0]\n    new_column.append(counter2)\ndf['most_used_word'] = new_column\nprint(df.head())\nprint(df['most_used_word'].unique())","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"56d0ec682b4d393e32f9a328ed57fa108c1d65bb"},"cell_type":"markdown","source":"Conclusion\n\nWe have identified and outline the key personality traits of different types of people. The biggest takeaways is that personality type does not predict success or future results. What it does predict is your strengths and weaknesses in terms of personality. And one of the things I have taken into consideration is that when we are creating teams to fight crime, develop amazing software, or simple play sports. It's important to consider everyone technical position on the team, but it's also deeply important to explore to soft skills to ensure strengths and weakness are balanced. Which allows another layer of interdisciplinary not only in technicall skill set, but also in mindset and personality. In short, this is just the first step into creating a personality type model based on meyers briggs assessment from social media comment data. The next step is to focus in one close domain use cases to solve real problems. \n![Personality](https://bj1oh303t6x351kzp35xea4o-wpengine.netdna-ssl.com/wp-content/uploads/2014/08/mbti-dating-infographic-section1.gif)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc6497098f378400f5b048a5b2e910a03ebddbe0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}